{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e7b202-3356-45ec-be57-d0a584dec25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install mysql_connector_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c205164-8d01-4296-84c6-c90cd2d58538",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, silhouette_score, mean_absolute_error, r2_score, classification_report, confusion_matrix, f1_score\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.cluster import KMeans \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d87e472-3ed4-4087-a930-d1662cb3ab26",
   "metadata": {},
   "source": [
    "# PRACTICAL QUESTIONS ON DATA MINING AND WAREHOUSING\n",
    "## 2.1 Introduction to Data Mining\n",
    "### Data Cleaning:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20da7d92-b966-45a3-8524-826ac794f54a",
   "metadata": {},
   "source": [
    "#### a. Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ca5940-b54f-4671-a73b-85827e7f4017",
   "metadata": {},
   "source": [
    "#### b. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5bd0dc0-17d5-4da6-aaf6-82d71a5d8b66",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'customer_transactions.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcustomer_transactions.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m df\u001b[38;5;241m.\u001b[39minfo()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'customer_transactions.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('customer_transactions.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bc1a9a-7669-475b-9210-69de27c57c23",
   "metadata": {},
   "source": [
    "#### c. Remove Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0298a2d-42a1-454d-8e30-fe9f591fe08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb7ab44-839a-4093-9ff0-cd1e27530db4",
   "metadata": {},
   "source": [
    "#### d. Handle Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57da0750-9f99-4bb1-8d72-e0bf1029267f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the numeric columns from the dataframe\n",
    "numeric_columns = df.select_dtypes(include=['number']).columns\n",
    "\n",
    "# Fill NaN value(s) with the median\n",
    "df[numeric_columns] = df[numeric_columns].fillna(df[numeric_columns].median())\n",
    "\n",
    "# Fill NaN in Non-numeric with a specific value\n",
    "df['date'] = df['date'].fillna('2023-01-01')\n",
    "\n",
    "df['product'] = df['product'].fillna('unknown')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a82288-bc9d-4c9f-8b69-dadc0046a41d",
   "metadata": {},
   "source": [
    "#### e. Convert date columns to standard format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbf917a-ddae-44b4-ae36-ca88b1df60a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae70eccc-cae9-4949-9695-6373c0cbb72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8691f7-7f4d-42b1-bdfd-e7b1d6c113e8",
   "metadata": {},
   "source": [
    "## Data Integration:\n",
    "• You have sales data from three different regions stored in separate CSV files. Combine these \n",
    "files into a single dataset using Python (pandas). Ensure that there are no duplicates and \n",
    "handle any discrepancies in the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4077b60-f965-4982-8331-538fd3bef026",
   "metadata": {},
   "source": [
    "#### sales data for region 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066e90fb-bc8a-4cb6-9d1a-a2ccf51a2310",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_region1 = {\n",
    "    'sales_id': [1, 2],\n",
    "    'product_id': [101, 102],\n",
    "    'customer_id': [1001, 1002],\n",
    "    'amount': [250, 450],\n",
    "    'date': ['2023-01-01', '2023-01-02']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771e9a91-70fc-4022-80a3-3d3a8a0b4733",
   "metadata": {},
   "source": [
    "#### sales data for region 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d808db7-4d19-4ebf-b095-4e437e3fcfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_region2 = {\n",
    "    'sales_id': [3, 4],\n",
    "    'product_id': [103, 104],\n",
    "    'customer_id': [1003, 1004],\n",
    "    'amount': [350, 200],\n",
    "    'date': ['2023-01-03', '2023-01-04']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e89b0a-5f10-4f35-85ba-bb998700920d",
   "metadata": {},
   "source": [
    "#### sales data for region 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db93463-3701-49ac-9c5d-bc38d630106e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_region3 = {\n",
    "    'sales_id': [5, 6],\n",
    "    'product_id': [105, 106],\n",
    "    'customer_id': [1005, 1006],\n",
    "    'amount': [300, 400],\n",
    "    'date': ['2023-01-05', '2023-01-06']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646a2414-d36f-409c-81f8-6b9caffa4c08",
   "metadata": {},
   "source": [
    "#### create the DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87159867-a6eb-4874-81a0-37b688fb2845",
   "metadata": {},
   "outputs": [],
   "source": [
    "region1_df = pd.DataFrame(sales_region1)\n",
    "region2_df = pd.DataFrame(sales_region2)\n",
    "region3_df = pd.DataFrame(sales_region3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a7c56e-255a-4c4a-ba32-c289d8a8dfcf",
   "metadata": {},
   "source": [
    "#### Write the DataFrames to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc447d6d-ca00-43c3-9aa0-c354cc5fa69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "region1_df.to_csv(\"sales_region1.csv\", index=False)\n",
    "region2_df.to_csv(\"sales_region2.csv\", index=False)\n",
    "region3_df.to_csv(\"sales_region3.csv\", index=False)\n",
    "\n",
    "print(\"CSV Files Created Successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ef87b4-1bac-4c9e-bfbf-b246b28126f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_region1 = pd.read_csv('sales_region1.csv')\n",
    "data_region2 = pd.read_csv('sales_region2.csv')\n",
    "data_region3 = pd.read_csv('sales_region3.csv')\n",
    "\n",
    "# Combine datasets\n",
    "combined_data = pd.concat([data_region1, data_region2, data_region3])\n",
    "\n",
    "# remove duplicates\n",
    "combined_data.drop_duplicates(inplace=True)\n",
    "\n",
    "# print the combined dataset\n",
    "combined_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b2b7f1-e949-45a3-b5b2-918dbe7f0360",
   "metadata": {},
   "source": [
    "## Data Selection:\n",
    "\n",
    "From a large dataset containing 10 years of customer purchase history, select data for the last 3 years \n",
    "for further analysis. Write SQL queries to:\n",
    "\n",
    "- Extract relevant records from a database.\n",
    "- Save the results into a new table or file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3796232-1246-4c83-b249-0cb395c1f6ea",
   "metadata": {},
   "source": [
    "## Data Transformation\n",
    "\n",
    "Given a dataset with categorical variables, transform these variables into numerical values suitable \n",
    "for machine learning models using Python (pandas, scikit-learn). Use one-hot encoding for nominal \n",
    "variables and label encoding for ordinal variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76deb5c0-e9b7-42a5-aa79-4a167a83ea73",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'customer_id': [1, 2, 3, 4, 5],\n",
    "    'purchase_id': [101, 102, 103, 104, 105],\n",
    "    'product': ['ProductA', 'ProductB', 'ProductC', 'ProductD', 'ProductE'],\n",
    "    'region': ['North', 'South', 'East', 'West', 'North'],\n",
    "    'satisfaction': ['Low', 'Medium', 'High', 'Low', 'High']\n",
    "}\n",
    "\n",
    "sales_df = pd.DataFrame(data)\n",
    "\n",
    "sales_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f5743c-7fe9-4fb4-adf6-912dff784830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding for 'region' (nominal variable)\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
    "region_encoded = one_hot_encoder.fit_transform(sales_df[['region']])\n",
    "\n",
    "# Create a DataFrame with one-hot-encoded variables\n",
    "region_encoded_df = pd.DataFrame(region_encoded, columns=one_hot_encoder.get_feature_names_out(['region'])) \n",
    "\n",
    "# Concatenate the encoded df with the original df\n",
    "df = pd.concat([sales_df, region_encoded_df], axis=1).drop('region', axis=1)\n",
    "\n",
    "df\n",
    "# region_encoded_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c08332a-115b-4288-9eb7-d9b66074a824",
   "metadata": {},
   "source": [
    "# 2.2 Data Mining Techniques\r",
    "## \n",
    "Clustering Techniques\n",
    ":\r\n",
    "Using the K-Means clustering algorithm, segment a dataset of customer profiles based on thei \r\n",
    "purchasing behavior. Use Python (scikit-learn) t- \r\n",
    "• Standardize the featur- .\r\n",
    "• Apply K-Means clustering with an appropriate number of clust- s.\r\n",
    "• Visualize the clusters using a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23146497-1143-443d-9419-001e761b847e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the customer profiles dataset\n",
    "# 10 sample customers\n",
    "# customer_id, age, gender, income, region, purchase_frequency \n",
    "\n",
    "data = {\n",
    "    'customer_id': [1, 2, 3, 4, 5],\n",
    "    'feature1': [35, 40, 50, 30, 45],\n",
    "    'feature2': [20000, 25000, 30000, 15000, 28000],\n",
    "}\n",
    "\n",
    "profiles = pd.DataFrame(data)\n",
    "\n",
    "profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa70723-27bc-4ece-9fbf-196cdc1a7216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize the features\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaled_data = scaler.fit_transform(profiles)\n",
    "\n",
    "# Apply KMeans clustering\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "kmeans.fit(scaled_data)\n",
    "profiles['Cluster'] = kmeans.labels_\n",
    "\n",
    "# Visualize the clusters\n",
    "plt.scatter(data['feature1'], data['feature2'], c=profiles['Cluster'])\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f452390-11c3-4fc5-b451-182926e0c979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHALLENGE\n",
    "\n",
    "# Create the customer profiles dataset\n",
    "# 10 sample customers\n",
    "# customer_id, age, gender, income, region, purchase_frequency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4b1b8c0-623c-407e-8a4c-3d2be6f13c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customer_profiles.csv has been created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Define the sample customer profiles data\n",
    "data = {\n",
    "    'customer_id': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'age': [25, 34, 28, 45, 38, 50, 29, 42, 33, 27],\n",
    "    'gender': ['F', 'M', 'F', 'M', 'F', 'M', 'F', 'M', 'F', 'M'],\n",
    "    'income': [50000, 75000, 62000, 80000, 90000, 70000, 55000, 72000, 65000, 48000],\n",
    "    'region': ['North', 'South', 'East', 'West', 'North', 'South', 'East', 'West', 'North', 'South'],\n",
    "    'purchase_frequency': [10, 15, 8, 20, 12, 18, 14, 17, 11, 16]\n",
    "}\n",
    "\n",
    "# Create a Customer Profiles DataFrame\n",
    "customer_profiles_df = pd.DataFrame(data)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "customer_profiles_df.to_csv('customer_profiles.csv', index=False)\n",
    "\n",
    "print(\"customer_profiles.csv has been created successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a83c3d-447e-4736-b27d-fa2cad0489be",
   "metadata": {},
   "source": [
    "## Project: Customer Segmentation using the K-Means clustering algorithm\n",
    "\n",
    "We'll use the customer_profiles.csv dataset we generated.\n",
    "\n",
    "Project Outline\n",
    "1. Introduction to K-Means Clustering\n",
    "2. Loading and Exploring the Dataset\n",
    "3. Data Preprocessing\n",
    "4. Applying K-Means Clustering\n",
    "5. Visualizing the Clusters\n",
    "6. Evaluating the Clustering\n",
    "7. Conclusion\n",
    "\n",
    "\n",
    "### 1. Introduction to K-Means Clustering\n",
    "K-Means Clustering is an unsupervised machine learning algorithm used to partition a dataset into K distinct, non-overlapping subsets (or clusters). The algorithm works as follows:\n",
    "1. Initialization: Select K initial centroids randomly.\n",
    "2. Assignment: Assign each data point to the nearest centroid, forming K clusters.\n",
    "3. Update: Calculate the new centroids as the mean of all data points assigned to each cluster.\n",
    "4. Repeat: Repeat steps 2 and 3 until the centroids no longer change or a maximum number of iterations is reached.\n",
    "\n",
    "### 2. Loading and Exploring the Dataset\n",
    "First, we need to load the dataset and explore its contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786e7697-1663-4713-a419-aa4ef8fff78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Data\n",
    "df = pd.read_csv('customer_profiles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b322d92e-f9d7-435c-a557-c0c6037b1a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of our dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88e7512-f46c-4b63-a618-862b280a1d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3003c4-b5fb-4397-a0b0-e9ff68641b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the Summary Statistics of the Data\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dfb91e-b965-41af-a6e2-88624fba606a",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing\n",
    "\n",
    "Before applying K-Means clustering, we need to preprocess the data. This includes handling missing values, encoding categorical variables, and scaling the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0c742a-e436-4739-934c-0d4f18a48742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2540a3-5b30-4d64-84f7-91a0424fd39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6ff649-ac5b-426f-9351-a3760ee6e1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variables (gender and region)\n",
    "df_encoded = pd.get_dummies(df, columns=['gender', 'region'])\n",
    "df_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc3855b-5274-452e-84c1-178686389a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale our data\n",
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(df_encoded.drop('customer_id', axis=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a81628-07df-4337-a7a0-5b1a9c00f444",
   "metadata": {},
   "source": [
    "## 4. Applying K-Means Clustering\n",
    "\n",
    "Now, we apply the K-Means clustering algorithm to the preprocessed data. We will also determine the optimal number of clusters using the Elbow Method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735a11ac-56ce-4a99-bb22-a80e6a735228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the optimal number of clusters using the Elbow method\n",
    "sse = []\n",
    "\n",
    "for k in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(df_scaled)\n",
    "    sse.append(kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4493602-3cc9-4502-8b5b-6d218a18ab9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Elbow Method Graph\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, 11), sse, marker='o')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Sum of Squared Errors')\n",
    "plt.title('Elbow Method For Determining Optimal Number of Clusters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8635ffd5-b9bd-4fff-b0fd-3e7f828ea9ba",
   "metadata": {},
   "source": [
    "Based on the Elbow Method, we choose the optimal number of clusters (lets say its 4) and apply K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecddb455-87fb-4b06-a92c-07cad574c4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply K-Means Clustering with the chosen number of clusters\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "customer_profiles_df['cluster'] = kmeans.fit_predict(df_scaled)\n",
    "\n",
    "# Display the first few rows of the dataset with cluster labels\n",
    "# customer_profiles_df.head()\n",
    "customer_profiles_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3772666-ba1a-4501-be1e-4a628a232d54",
   "metadata": {},
   "source": [
    "## 5. Visualizing the Clusters\n",
    "\n",
    "We can visualize the clusters using a pair plot for better understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c8b1b5-e4bb-48a0-9f54-a22b538690fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(customer_profiles_df, hue=\"cluster\", palette=\"viridis\", diag_kind='kde')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513b61b7-6459-4341-9eac-ed1192db50b0",
   "metadata": {},
   "source": [
    "## 6. Evaluating the Clustering\n",
    "\n",
    "We evaluate the clustering using silhouette score, which measures how similar a data point is to its own cluster compared to other clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885bec18-2636-40b7-bef3-6fa6d0851379",
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_avg = silhouette_score(df_scaled, customer_profiles_df['cluster'])\n",
    "print(f\"\\nSilhouette Score: {silhouette_avg:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71724880-2afe-4b18-81b2-cd990782a160",
   "metadata": {},
   "source": [
    "## 7. Conclusion\n",
    "The K-Means clustering algorithm has successfully segmented the customers into distinct clusters based on their profiles. This segmentation can be used for targeted marketing, personalized offers, and improved customer service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de9bc1e-d0ba-421d-8f99-bd1ef6739ce5",
   "metadata": {},
   "source": [
    "## Explanation of Concepts\n",
    "\n",
    "- Data Preprocessing: This step involves transforming raw data into a suitable format for analysis. This includes encoding categorical variables (converting categories into numerical values) and scaling numerical features to have a mean of 0 and a standard deviation of 1.\n",
    "- K-Means Clustering: An algorithm that partitions data into K clusters by minimizing the variance within each cluster.\n",
    "- Elbow Method: A technique to determine the optimal number of clusters by plotting the sum of squared errors (SSE) against the number of clusters and identifying the \"elbow point\" where the SSE starts to decrease at a slower rate.\n",
    "- Silhouette Score: A measure of how similar an object is to its own cluster compared to other clusters. It ranges from -1 to 1, with higher values indicating better clustering.\n",
    "- Pair Plot: A visualization technique to plot pairwise relationships in a dataset. It helps in understanding the distribution and relationships between different variables in the context of clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac882a1-cc45-4280-8b1b-b75afac27ee3",
   "metadata": {},
   "source": [
    "## Association Rule Mining:\n",
    "Analyze a dataset of retail transactions to find frequent itemsets and generate association rules using \n",
    "the Apriori algorithm. Use Python (mlxtend) to:\n",
    "- Identify itemsets with a minimum support of 0.02.\n",
    "- Generate association rules with a minimum confidence of 0.5.\n",
    "- Interpret the top 5 association rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fbe570-2b36-4c13-bbe8-2ea76001b813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the transactions dataset\n",
    "data = {\n",
    "    'transaction_id': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'milk': [1, 0, 1, 1, 0, 1, 0, 0, 1, 1],\n",
    "    'bread': [1, 1, 1, 0, 0, 1, 1, 0, 1, 0],\n",
    "    'butter': [0, 1, 0, 1, 0, 0, 1, 1, 0, 1],\n",
    "    'cheese': [0, 0, 1, 0, 1, 1, 0, 1, 1, 0],\n",
    "    'apples': [1, 0, 0, 1, 1, 0, 1, 0, 1, 1],\n",
    "    'bananas': [0, 1, 1, 1, 0, 1, 1, 0, 0, 0]\n",
    "}\n",
    "transactions_df = pd.DataFrame(data)\n",
    "transactions_df.to_csv('transactions.csv', index=False)\n",
    "print(\"transactions.csv has been created succesfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b5d831-18b0-4452-8303-6a0cd11ac9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "transactions_df = pd.read_csv('transactions.csv')\n",
    "\n",
    "transactions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dfe465-0cc6-4c00-b6ed-0ef0988eb113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the transaction_id column\n",
    "transactions = transactions_df.drop('transaction_id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e61ef6-2096-4205-86a9-141caef07c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the DataFrame to a list of list format\n",
    "transactions_list = transactions.apply(lambda x: transactions.columns[x == 1].tolist(), axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8625169c-da93-4f5c-b8a8-909ea0a4e1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e06e8f8-3dbd-4600-b5f7-4e96a38dd936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use TransactionEncoder to transform the data\n",
    "te = TransactionEncoder()\n",
    "te_arr = te.fit(transactions_list).transform(transactions_list)\n",
    "\n",
    "encoded_transactions = pd.DataFrame(te_arr, columns=te.columns_)\n",
    "\n",
    "encoded_transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639f7875-84e9-4349-9bde-b00d39867290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate association rules with a minimum support of 0.02\n",
    "# Apply the Apriori Algorithm\n",
    "frequent_itemsets = apriori(encoded_transactions, min_support=0.02, use_colnames=True)\n",
    "frequent_itemsets = frequent_itemsets.sort_values(by='support', ascending=False)\n",
    "\n",
    "frequent_itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad4264f-5931-4813-9a47-0e5df37523af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate association rules with a minimum confidence of 0.5\n",
    "# Generate association rules\n",
    "rules = association_rules(frequent_itemsets,metric='confidence', min_threshold=0.5)\n",
    "rules = rules.sort_values(by='confidence', ascending=False)\n",
    "\n",
    "rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd585d69-1564-43d2-af29-c74645e603d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpret the top 5 association rules\n",
    "top_5_rules = rules.head()\n",
    "top_5_rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9756fac1-9f4c-4977-9230-1f18d14c14b5",
   "metadata": {},
   "source": [
    "The top_5_rules DataFrame will contain the following columns:\n",
    "- antecedents: The itemsets on the left-hand side of the rule.\n",
    "- consequents: The itemsets on the right-hand side of the rule.\n",
    "- support: The support of the rule.\n",
    "- confidence: The confidence of the rule.\n",
    "- lift: The lift of the rule."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c661a9-def5-49da-bf6b-37cc35438b39",
   "metadata": {},
   "source": [
    "## Decision Trees and Random Forests:\n",
    "Build a decision tree model to predict customer churn using a given dataset. Use Python (scikit-learn) \n",
    "to:\n",
    "- Split the dataset into training and testing sets.\n",
    "- Train a decision tree classifier.\n",
    "- Evaluate the model’s performance using accuracy, precision, and recall.\n",
    "- Extend the above problem by using a random forest classifier. Compare its performance with \n",
    "the decision tree model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95497f53-fadf-4e0c-9fd1-4c59034b295f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sample data for customer_churn.csv\n",
    "\n",
    "customer_churn_data = {\n",
    "    'customer_id': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'age': [25, 34, 28, 45, 38, 50, 29, 42, 33, 27],\n",
    "    'gender': ['F', 'M', 'F', 'M', 'F', 'M', 'F', 'M', 'F', 'M'],\n",
    "    'income': [50000, 75000, 62000, 80000, 90000, 70000, 55000, 72000, 65000, 48000],\n",
    "    'region': ['North', 'South', 'East', 'West', 'North', 'South', 'East', 'West', 'North', 'South'],\n",
    "    'purchase_frequency': [10, 15, 8, 20, 12, 18, 14, 17, 11, 16],\n",
    "    'churn': [0, 1, 0, 1, 0, 1, 0, 1, 0, 1]  # 0: Not churned, 1: Churned\n",
    "}\n",
    "\n",
    "# create a DataFrame\n",
    "customer_churn_df = pd.DataFrame(customer_churn_data)\n",
    "\n",
    "# save the DataFrame to a csv file\n",
    "customer_churn_df.to_csv('customer_churn.csv', index=False)\n",
    "\n",
    "print(\"customer_churn.csv has been created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c7f145-d75e-4554-b180-f547258fda6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e76174f-3010-4520-85af-22eebff9faba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Dataset\n",
    "customer_churn_df = pd.read_csv('customer_churn.csv')\n",
    "\n",
    "# Convert categorical variables to numerical values\n",
    "customer_churn_df['gender'] = customer_churn_df['gender'].map({'M': 0, 'F': 1})\n",
    "\n",
    "# Encode categorical variables (gender and region)\n",
    "df_encoded = pd.get_dummies(customer_churn_df.drop('customer_id', axis=1), drop_first=True)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X = df_encoded.drop('churn', axis=1)\n",
    "y = df_encoded['churn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c19c9e-b0a3-4e9c-adb6-3e7cab76d528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the Dataset into Training and Testing Sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab68ae09-cdfd-46b6-87c8-44d09f406bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a DecisionTreeClassifier\n",
    "decision_tree = DecisionTreeClassifier(random_state=42)\n",
    "decision_tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7854f86f-9120-42eb-8c1a-8f7d4d6e54a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "y_pred_dt = decision_tree.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9cf93f-423b-4325-9605-7ede919afb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7dc92c-da90-426d-b9b0-8da6147cb9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the Decision Tree Model\n",
    "accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
    "precision_dt = precision_score(y_test, y_pred_dt, zero_division=1)\n",
    "recall_dt = recall_score(y_test, y_pred_dt)\n",
    "print(\"Decision Tree Classifier:\")\n",
    "print(f\"Accuracy: {accuracy_dt:.2f}\")\n",
    "print(f\"Precision: {precision_dt:.2f}\")\n",
    "print(f\"Recall: {recall_dt:.2f}\")\n",
    "print(classification_report(y_test, y_pred_dt, zero_division=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa27388-27e9-456d-b711-545ef64933ea",
   "metadata": {},
   "source": [
    "### Evaluation Metrics:\n",
    "- Accuracy: The ratio of correctly predicted instances to the total instances. It is a good measure when the classes are balanced.\n",
    "- Precision: The ratio of correctly predicted positive observations to the total predicted positives. It is a useful measure when the costs of false positives are high.\n",
    "- Recall: The ratio of correctly predicted positive observations to the all observations in actual class. It is a useful measure when the costs of false negatives are high.\n",
    "These metrics are computed using Scikit-learn functions: accuracy_score, precision_score, and recall_score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1c905d-f361-445c-baa3-f64e3ec79df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Random Forest Classifier\n",
    "random_forest = RandomForestClassifier(random_state=42)\n",
    "random_forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55696ad2-af3f-49f9-96b5-7aa1fca2e264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "y_pred_rf = random_forest.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf34846c-c767-4e94-a1b8-09405e5b2b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the Random Forest Model\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "precision_rf = precision_score(y_test, y_pred_rf, zero_division=1)\n",
    "recall_rf = recall_score(y_test, y_pred_rf)\n",
    "\n",
    "print(\"Random Forest Classifier:\")\n",
    "print(f\"Accuracy: {accuracy_rf:.2f}\")\n",
    "print(f\"Precision: {precision_rf:.2f}\")\n",
    "print(f\"Recall: {recall_rf:.2f}\")\n",
    "print(classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfcee52-7e19-4612-bb63-f6929140c038",
   "metadata": {},
   "source": [
    "## Regression Analysis:\n",
    "Using a dataset of housing prices, perform linear regression to predict the price of a house based on \n",
    "its features (e.g., number of bedrooms, square footage). Use Python (scikit-learn) to:\n",
    "- Preprocess the data by handling missing values and scaling features.\n",
    "- Train a linear regression model.\n",
    "- Evaluate the model using metrics such as Mean Absolute Error (MAE) and R-squared."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9f56da-793a-42a2-b5e4-dc6fa704c1c9",
   "metadata": {},
   "source": [
    "### Linear Regression for Housing Price Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea4636e-eca1-4db4-8c8d-04ceeb6031ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sample data for housing_prices.csv with cities in Kenya\n",
    "housing_prices_data = {\n",
    "    'house_id': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'location': ['Nairobi', 'Mombasa', 'Kisumu', 'Nakuru', 'Eldoret', 'Thika', 'Kitale', 'Malindi', 'Garissa', 'Naivasha'],\n",
    "    'size_sqft': [2000, 2500, 1800, 2200, 2400, 2300, 2100, 2600, 2700, 1900],\n",
    "    'bedrooms': [3, 4, 3, 4, 4, 4, 3, 5, 4, 3],\n",
    "    'price': [5000000, 7500000, 4000000, 6000000, 7200000, 6800000, 4500000, 7800000, 8000000, 4200000]\n",
    "}\n",
    "\n",
    "# create a DataFrame\n",
    "housing_prices_df = pd.DataFrame(housing_prices_data)\n",
    "\n",
    "# save Data set to csv\n",
    "housing_prices_df.to_csv('housing_prices.csv',index=False)                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f8087f-f643-4315-ae4f-6aea4f9040ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Dataset\n",
    "housing_prices_df = pd.read_csv('housing_prices.csv')\n",
    "\n",
    "# Convert categorical variables to numerical values\n",
    "housing_prices_df = pd.get_dummies(housing_prices_df, columns=['location'], drop_first=True)\n",
    "housing_prices_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bae01da-d445-449b-9501-f1ead7f198ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target variable\n",
    "X = housing_prices_df.drop(['house_id', 'price'], axis=1)\n",
    "y = housing_prices_df['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad905096-ca89-4698-8b34-f9495d6a42b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "# Handle missing values\n",
    "X.fillna(X.mean(), inplace=True)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9036e5-f096-4f69-a272-5d33f37c3709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the Dataset into Training and Testing Sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33937c48-d1b8-4a64-a591-76805d8941f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Linear Regression Model\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a8732c-2671-4ef1-abb3-5d3eb24e35f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "y_pred_lr = lr_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c392622d-a1de-4012-9fc3-f14c95bffd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the Linear Regression Model\n",
    "mae_lr = mean_absolute_error(y_test, y_pred_lr)\n",
    "r2_lr = r2_score(y_test, y_pred_lr)\n",
    "\n",
    "print(\"Linear Regression Model\")\n",
    "print(\"Mean Absolute Error (MAE):\", mae_lr)\n",
    "print(\"R-squared (R2):\", r2_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e5a70e-b4d9-4a5a-8928-0be97508275a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization with one feature\n",
    "# Here we assume 'square_footage' is the first column for simplicity\n",
    "# In practice, replace 'square_footage' with the actual feature name\n",
    "feature_index = 0\n",
    "feature_name = X.columns[feature_index]\n",
    "\n",
    "# Scatter plot of the actual values vs. the predicted values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_test[:, feature_index], y_test, color='blue', label='Actual Prices')\n",
    "plt.scatter(X_test[:, feature_index], y_pred_lr, color='red', label='Predicted Prices')\n",
    "plt.xlabel(feature_name)\n",
    "plt.ylabel('Price')\n",
    "plt.title('Actual vs Predicted Prices')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74f9bc0-85a1-487e-b6fd-85d0902b45c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual plot\n",
    "residuals = y_test - y_pred_lr\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.residplot(x=y_pred_lr, y=residuals, lowess=True)\n",
    "plt.xlabel('Predicted Prices')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals vs Predicted Prices')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078cf385-d618-4785-8491-ec2a5768dabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting regression line with one feature\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.regplot(x=X_test[:, feature_index], y=y_test, ci=None, scatter_kws={\"color\": \"blue\"}, line_kws={\"color\": \"red\"})\n",
    "plt.xlabel(feature_name)\n",
    "plt.ylabel('Price')\n",
    "plt.title(f'Regression Line for {feature_name}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59a89c9-24fa-4972-82e3-11804a833014",
   "metadata": {},
   "source": [
    "### Support Vector Machines (SVM):\r\n",
    "Classify emails as spam or non-spam using a given dataset. Use Python (scikit-learn) to:\n",
    "- • Convert the text data into numerical features using TF-IDF vectorization\n",
    "- \n",
    "• Train an SVM classifie\n",
    "- \r\n",
    "• Evaluate the model’s performance using a confusion matrix and F1-score.ore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6709bfe6-9a1b-4d15-88ce-78b420a6fb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the email dataset\n",
    "\n",
    "# Sample email data\n",
    "data = {\n",
    "    'email_text': [\n",
    "        \"Hey, let's catch up soon. How about this weekend?\",\n",
    "        \"Congratulations, you have won a lottery of $1 million!\",\n",
    "        \"Meeting scheduled for tomorrow at 10 AM.\",\n",
    "        \"Your account has been compromised. Please click the link to reset your password.\",\n",
    "        \"Don't miss out on our exclusive offers!\",\n",
    "        \"Please find the attached report for your review.\",\n",
    "        \"Get a free trial of our service for one month.\",\n",
    "        \"Thank you for your purchase! Your order is being processed.\",\n",
    "        \"Urgent! Your immediate action is required.\",\n",
    "        \"Happy Birthday! Wishing you all the best.\"\n",
    "    ],\n",
    "    'label': [\n",
    "        0,  # 0: Ham\n",
    "        1,  # 1: Spam\n",
    "        0,  # 0: Ham\n",
    "        1,  # 1: Spam\n",
    "        1,  # 1: Spam\n",
    "        0,  # 0: Ham\n",
    "        1,  # 1: Spam\n",
    "        0,  # 0: Ham\n",
    "        1,  # 1: Spam\n",
    "        0   # 0: Ham\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "email_df = pd.DataFrame(data)\n",
    "\n",
    "# Save to CSV\n",
    "email_df.to_csv('emails.csv', index=False)\n",
    "\n",
    "print(\"emails.csv has been created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf81a7a5-7d91-455f-894b-a01170dd9238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Dataset\n",
    "data = pd.read_csv('emails.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeba7989-3345-486c-b8d6-21c2361fa26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text data into numerical features\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "X = vectorizer.fit_transform(data['email_text'])\n",
    "y = data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64bc26a-14b0-4055-9f51-2dc3990ecbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2154d099-46bd-4755-97d6-808d2953f213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the SVM Classifier\n",
    "model = SVC()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9726e5b5-3fbd-465b-8f1a-a6b1d0a2c366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5d0836-d151-466f-89e7-2e51b2b014cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Confusion Matrix: {confusion_matrix(y_test, y_pred)}\")\n",
    "print(f\"F1-Score: {f1_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69cb080-4145-436d-a98f-e4d3c0b7af11",
   "metadata": {},
   "source": [
    "Designing and Implementing a Data Warehouse\n",
    "Data Warehouse Modelling:\n",
    "\n",
    "Star Schema Design:\n",
    "\n",
    "Fact Table:\n",
    "Sales: sales_id, product_id, customer_id, time_id, quantity_sold, sales_amount.\n",
    "\n",
    "Dimension Tables:\n",
    "Product: product_id, product_name, category, price.\n",
    "Customer: customer_id, customer_name, location, age.\n",
    "Time: time_id, date, month, quarter, year.\n",
    "\n",
    "ER Diagram: Can be drawn using tools like Lucidchart or Microsoft Visio.\n",
    "\n",
    "Dimensional Modelling:\n",
    "\n",
    "Snowflake Schema Design:\n",
    "\n",
    "Product Dimension:\n",
    "Product: product_id, product_name, category_id, price.\n",
    "Category: category_id, category_name.\n",
    "Customer Dimension:\n",
    "Customer: customer_id, customer_name, location_id, age.\n",
    "Location: location_id, city, state, country.\n",
    "Advantages and Disadvantages:\n",
    "\n",
    "Advantages: Reduces redundancy, saves storage space.\n",
    "Disadvantages: Can be complex to navigate, may require more joins in queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b4a67f-f877-4ca9-9570-bb9790f4bcc9",
   "metadata": {},
   "source": [
    "### ETL Process:\n",
    "Implement an ETL process to extract data from CSV files, transform it to a suitable format, and load it \n",
    "into a MySQL database. Write a Python script using libraries such as pandas and SQLAlchemy to:\n",
    "Extract data from multiple CSV files.\n",
    "\n",
    "Transform the data (e.g., handle missing values, normalize features).\n",
    "\n",
    "Load the data into a MySQL database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d246b3ba-fc16-4783-a915-b86b4dc6a3bd",
   "metadata": {},
   "source": [
    "#### Database Connection Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50835650-60f2-4895-b032-37b579d21fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_username = 'root'\n",
    "db_password = ''\n",
    "db_host = 'localhost'\n",
    "db_name = 'star_schema_db'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7ce5ca-b779-44b1-83c8-5b34a4c28745",
   "metadata": {},
   "source": [
    "#### Establish a connection to MySQL Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bbed17-a51c-4d5c-a09d-45b01c76b1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(f'mysql+mysqlconnector://{db_username}:{db_password}@{db_host}/{db_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607e4e12-9292-4fad-8100-df0a6f857d8f",
   "metadata": {},
   "source": [
    "#### Create the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94a468f-f35c-48b7-8fb8-49ac5dcdeae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for customers.csv\n",
    "customers_data = {\n",
    "    'customer_id': [1, 2, 3, 4, 5],\n",
    "    'customer_name': ['John Doe', 'Jane Smith', 'Emily Davis', 'Michael Brown', 'Jessica Wilson'],\n",
    "    'location': ['New York', 'California', 'Texas', 'Florida', 'Nevada'],\n",
    "    'age': [28, 34, 29, 45, 23]\n",
    "}\n",
    "customers_df = pd.DataFrame(customers_data)\n",
    "customers_df.to_csv('customers.csv', index=False)\n",
    "print(\"customers.csv has been created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631166a0-64b7-4b29-89b6-905def9c3682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for products.csv\n",
    "products_data = {\n",
    "    'product_id': [101, 102, 103, 104, 105],\n",
    "    'product_name': ['Laptop', 'Smartphone', 'Tablet', 'Monitor', 'Keyboard'],\n",
    "    'category': ['Electronics', 'Electronics', 'Electronics', 'Electronics', 'Accessories'],\n",
    "    'price': [1000, 500, 300, 200, 50]\n",
    "}\n",
    "products_df = pd.DataFrame(products_data)\n",
    "products_df.to_csv('products.csv', index=False)\n",
    "print(\"products.csv has been created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c06aa0-893f-4346-ba42-ca000db00492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for sales.csv\n",
    "sales_data = {\n",
    "    'sales_id': [1001, 1002, 1003, 1004, 1005],\n",
    "    'product_id': [101, 102, 103, 104, 105],\n",
    "    'customer_id': [1, 2, 3, 4, 5],\n",
    "    'time_id': [202301, 202302, 202303, 202304, 202305],\n",
    "    'quantity_sold': [2, 1, 3, 1, 4],\n",
    "    'sales_amount': [2000, 500, 900, 200, 200]\n",
    "}\n",
    "sales_df = pd.DataFrame(sales_data)\n",
    "sales_df.to_csv('sales.csv', index=False)\n",
    "print(\"sales.csv has been created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e616c9f-9e82-4b30-b1f5-48c972380bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for time.csv\n",
    "time_data = {\n",
    "    'time_id': [202301, 202302, 202303, 202304, 202305],\n",
    "    'date': ['2023-01-01', '2023-02-01', '2023-03-01', '2023-04-01', '2023-05-01'],\n",
    "    'month': ['January', 'February', 'March', 'April', 'May'],\n",
    "    'quarter': ['Q1', 'Q1', 'Q1', 'Q2', 'Q2'],\n",
    "    'year': [2023, 2023, 2023, 2023, 2023]\n",
    "}\n",
    "time_df = pd.DataFrame(time_data)\n",
    "time_df.to_csv('time.csv', index=False)\n",
    "print(\"time.csv has been created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f78c7f-41d3-45b3-9b7a-504f20059323",
   "metadata": {},
   "source": [
    "#### Extract Data from CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b2105f-51a5-4b41-a8f6-f1281a83a456",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_df = pd.read_csv('customers.csv')\n",
    "products_df = pd.read_csv('products.csv')\n",
    "sales_df = pd.read_csv('sales.csv')\n",
    "time_df = pd.read_csv('time.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e0e208-e14e-4873-9b0a-0d1dfa591e8c",
   "metadata": {},
   "source": [
    "#### transform the data (Handling missing values and normalise features) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27fd173-8d79-44ee-b3b1-a677535319bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling Missing Vlaues by filling with median\n",
    "customers_df['age'].fillna(customers_df['age'].median(), inplace=True)\n",
    "products_df['price'].fillna(products_df['price'].median(), inplace=True)\n",
    "sales_df['quantity_sold'].fillna(sales_df['quantity_sold'].median(), inplace=True)\n",
    "sales_df['sales_amount'].fillna(sales_df['sales_amount'].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c94dd7-1d62-49ef-a985-266a9585978b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize numerical features (if necessary)\n",
    "customers_df['age'] = scaler.fit_transform(customers_df[['age']])\n",
    "products_df['price'] = scaler.fit_transform(products_df[['price']])\n",
    "sales_df[['quantity_sold', 'sales_amount']] = scaler.fit_transform(sales_df[['quantity_sold', 'sales_amount']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7824e4-4e51-4f08-a5b8-9e83c8081974",
   "metadata": {},
   "source": [
    "#### Load Data into MySQl Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de1498d-31f7-49e5-956b-d726a6b76377",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_df.to_sql('customers', con=engine, if_exists='replace', index=False)\n",
    "products_df.to_sql('products', con=engine, if_exists='replace', index=False)\n",
    "sales_df.to_sql('sales', con=engine, if_exists='replace', index=False)\n",
    "time_df.to_sql('time', con=engine, if_exists='replace', index=False)\n",
    "\n",
    "print(\"Data has been successfully loaded into the MySQL Database\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
